{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import data_preprocess\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "from model import SRCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for peak signal-to-noise ratio (PSNR)\n",
    "def psnr(target, ref):\n",
    "    # Convert images to torch tensors\n",
    "    target_data = torch.tensor(target, dtype=torch.float)\n",
    "    ref_data = torch.tensor(ref, dtype=torch.float)\n",
    "\n",
    "    # Calculate the squared difference between the two images\n",
    "    diff = ref_data - target_data\n",
    "    mse = torch.mean(diff ** 2)\n",
    "\n",
    "    # Calculate PSNR\n",
    "    psnr_value = 20 * torch.log10(255. / torch.sqrt(mse))\n",
    "    return psnr_value.item()\n",
    "\n",
    "# Define function for mean squared error (MSE)\n",
    "def mse(target, ref):\n",
    "    # Convert images to torch tensors\n",
    "    target_data = torch.tensor(target, dtype=torch.float)\n",
    "    ref_data = torch.tensor(ref, dtype=torch.float)\n",
    "\n",
    "    # Calculate the squared difference between the two images\n",
    "    diff = ref_data - target_data\n",
    "    mse_value = torch.mean(diff ** 2)\n",
    "\n",
    "    return mse_value.item()\n",
    "\n",
    "# Define function that combines all three image quality metrics\n",
    "def compare_images(target, ref):\n",
    "    scores = []\n",
    "    scores.append(psnr(target, ref))\n",
    "    scores.append(mse(target, ref))\n",
    "    return scores\n",
    "\n",
    "psnr_value = psnr(original_image, lr_img)\n",
    "mse_value = mse(original_image, lr_img)\n",
    "comparison_score = compare_images(original_image, lr_img)\n",
    "\n",
    "# psnr_value_patch = psnr(original_image, lr)\n",
    "# mse_value_patch = mse(original_image, lr)\n",
    "\n",
    "print(\"PSNR between HR and LR images:\", psnr_value)\n",
    "print(\"MSE between HR and LR images:\", mse_value)\n",
    "print(\"Comparison scores :\", comparison_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three images side by side\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Convert the RGB image to grayscale\n",
    "gray_image_original = Image.fromarray(original_image).convert('L')\n",
    "gray_image_lr = Image.fromarray(lr_img).convert('L')\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(gray_image_original, cmap='gray')\n",
    "plt.title('Original HR Image(Grayscale)')\n",
    "plt.axis('off')\n",
    "plt.text(200, 650, f'Size: {original_image.shape[0]} x {original_image.shape[1]}', color='black')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(gray_image_lr, cmap='gray')\n",
    "plt.title('LR Image (Grayscale)')\n",
    "plt.axis('off')\n",
    "plt.text(200, 650, f'Size: {lr_img.shape[0]} x {lr_img.shape[1]}', color='black')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(hr, cmap='gray')\n",
    "plt.title('High Resolution Patch')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(lr, cmap='gray')\n",
    "plt.title('Low Resolution Patch')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load an image and its corresponding HR image\n",
    "image_path = \"/content/drive/MyDrive/Retinal_Images_Dataset/original-images/im0001.ppm\"\n",
    "lr_patch, hr_patch = prepare_train_data(image_path)\n",
    "original_image = plt.imread(image_path)\n",
    "# print(lr_patch.shape)\n",
    "# print(hr_patch.shape)\n",
    "shape = original_image.shape\n",
    "\n",
    "scale = 8\n",
    "\n",
    "lr_img = cv2.resize(original_image, (int(shape[1] / scale), int(shape[0] / scale))) #shape[1] is the width of the original (HR) image, shape[0] is the height of the original image.\n",
    "# print(\"Size of LR Image after downsampling:\", lr_img.shape)\n",
    "lr_img = cv2.resize(lr_img, (shape[1], shape[0]))\n",
    "# print(\"Size of LR Image after resizing to size of HR Image:\", lr_img.shape)\n",
    "\n",
    "# Select one LR patch and its corresponding HR patch\n",
    "index = 20\n",
    "lr = lr_patch[index].squeeze().numpy()\n",
    "hr = hr_patch[index].squeeze().numpy()\n",
    "lr_size = lr.shape\n",
    "hr_size = hr.shape\n",
    "\n",
    "print(\"Shape of Original HR Image:\", original_image.shape)\n",
    "print(\"Shape of LR Image:\", lr_img.shape)\n",
    "print(\"Size of High Resolution Patch:\", hr_size)\n",
    "print(\"Size of Low Resolution Patch:\", lr_size)\n",
    "print(\"\")\n",
    "\n",
    "# Plot all three images side by side\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image)\n",
    "plt.title('Original HR Image')\n",
    "plt.axis('off')\n",
    "plt.text(200, 650, f'Size: {original_image.shape[0]} x {original_image.shape[1]}', color='black')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(lr_img)\n",
    "plt.title('LR Image')\n",
    "plt.axis('off')\n",
    "plt.text(200, 650, f'Size: {lr_img.shape[0]} x {lr_img.shape[1]}', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Super Resolution Convolution Neural Network (SRCNN) Model\n",
    "1) Explain the achitecture and hyper parameters of the SRCNN network from the original paper\n",
    "2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRCNN(\n",
      "  (conv1): Conv2d(1, 128, kernel_size=(9, 9), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 1, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = SRCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "Sample 0:\n",
      "Data Shape: (1, 32, 32)\n",
      "Label Shape: (1, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file = h5py.File(file_path, 'r')\n",
    "        self.data = self.file['data']\n",
    "        self.label = self.file['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx][0]  # Squeeze the batch dimension\n",
    "        label = self.label[idx][0]  # Squeeze the batch dimension\n",
    "        return data, label\n",
    "\n",
    "train_dataset = HDF5Dataset('train_data.h5')\n",
    "test_dataset = HDF5Dataset('test_data.h5')\n",
    "\n",
    "# View contents of train_dataset\n",
    "print(\"Train Dataset:\")\n",
    "for i in range(len(train_dataset)):\n",
    "    data, label = train_dataset[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(\"Data Shape:\", data.shape)\n",
    "    print(\"Label Shape:\", label.shape)\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input Shape: torch.Size([1, 1, 32, 32]), Batch Label Shape: torch.Size([1, 1, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "# Print the shape of batches in train_loader\n",
    "for inputs, labels in train_loader:\n",
    "    print(f\"Batch Input Shape: {inputs.shape}, Batch Label Shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Move optimizer state to the same device as the model i.e in GPU\n",
    "optimizer_state_dict = optimizer.state_dict()\n",
    "for key in optimizer_state_dict['state'].keys():\n",
    "    for param_key in optimizer_state_dict['state'][key].keys():\n",
    "        optimizer_state_dict['state'][key][param_key] = optimizer_state_dict['state'][key][param_key].to(device)\n",
    "optimizer.load_state_dict(optimizer_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "num_epochs = 30\n",
    "checkpoint_interval = 10  # Save checkpoint every 10 epochs\n",
    "checkpoint_dir = 'checkpoints'\n",
    "\n",
    "# Ensure checkpoint directory exists\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Move inputs and labels to the device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to prevent exploding gradients\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(running_loss / len(train_loader))\n",
    "\n",
    "    # Get the current learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    # Print epoch details and loss after each epoch\n",
    "    print(f'[Epoch {epoch + 1}] loss: {running_loss / len(train_loader):.4f}, Learning Rate: {current_lr:.4f}')\n",
    "\n",
    "    # Save checkpoint every checkpoint_interval epochs\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss / len(train_loader)\n",
    "        }, checkpoint_path)\n",
    "        print(f'Saved checkpoint at epoch {epoch + 1} to {checkpoint_path}')\n",
    "\n",
    "    # Handle resuming training from the latest checkpoint\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        latest_checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        if os.path.exists(latest_checkpoint_path):\n",
    "            checkpoint = torch.load(latest_checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(f'Resumed training from checkpoint at epoch {checkpoint[\"epoch\"]}')\n",
    "\n",
    "# Save final model weights\n",
    "final_model_weights_path = 'imageSuper-resolution_model_weights.pth'\n",
    "torch.save(model.state_dict(), final_model_weights_path)\n",
    "print(f'Final model weights saved to {final_model_weights_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SRCNN()\n",
    "model.load_state_dict(torch.load('imageSuper-resolution_model_weights.pth'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define lists to store evaluation metrics\n",
    "psnr_scores = []\n",
    "mse_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        for i in range(len(outputs)):\n",
    "            output_img = outputs[i].squeeze().cpu().numpy()  # Convert tensor to numpy array\n",
    "            label_img = labels[i].squeeze().cpu().numpy()   # Convert tensor to numpy array\n",
    "            scores = compare_images(output_img, label_img)  # Calculate PSNR and MSE\n",
    "            psnr_scores.append(scores[0])  # Append PSNR score\n",
    "            mse_scores.append(scores[1])   # Append MSE score\n",
    "\n",
    "avg_psnr = np.mean(psnr_scores)\n",
    "avg_mse = np.mean(mse_scores)\n",
    "print(f\"Average PSNR: {avg_psnr}\")\n",
    "print(f\"Average MSE: {avg_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = SRCNN()\n",
    "model.load_state_dict(torch.load('imageSuper-resolution_model_weights.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prepare test data\n",
    "test_data = []\n",
    "test_label = []\n",
    "for data, label in test_loader:\n",
    "    # Move data and label to the device\n",
    "    data, label = data.to(device), label.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(data)\n",
    "\n",
    "    # Append data and label to the list\n",
    "    test_data.append(data.cpu().numpy())\n",
    "    test_label.append(label.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "test_data = np.concatenate(test_data)\n",
    "test_label = np.concatenate(test_label)\n",
    "\n",
    "# Visualize some of the test images and their super-resolved counterparts\n",
    "num_images_to_visualize = 5\n",
    "for i in range(num_images_to_visualize):\n",
    "    # Get LR, HR, and SR images\n",
    "    lr_img = test_data[i].squeeze()\n",
    "    hr_img = test_label[i].squeeze()\n",
    "    sr_img = outputs[i].squeeze().cpu().detach().numpy()\n",
    "\n",
    "    # Display images\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(lr_img, cmap='gray')\n",
    "    plt.title('Low Resolution Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(hr_img, cmap='gray')\n",
    "    plt.title('High Resolution Image (Ground Truth)')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(sr_img, cmap='gray')\n",
    "    plt.title('Super-Resolved Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
